{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb99155",
   "metadata": {},
   "source": [
    "### 基本定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b250591",
   "metadata": {},
   "source": [
    "- 随机变量\n",
    "  - 对于一个箱子里面由三种颜色的球（红、黄、蓝），那么红黄蓝球就是随机变量\n",
    "- 随机抽样\n",
    "  - 每次从箱子里面随机抽一个球，称为随机抽样\n",
    "- 观测值\n",
    "  - 抽出来的是蓝球，那么蓝球就是观测值\n",
    "- 概率质量函数（PMF）\n",
    "  - 描述一个**离散概率分布**，例如抛硬币，随机变量X的取值范围是x={0,1}\n",
    "  - X的概率质量函数就是：$p(0) = \\frac {1}{2},p(1)=\\frac{1}{2}$\n",
    "- 概率密度函数(PDF)\n",
    "  - 描述一个连续概率分布，例如正态分布\n",
    "\n",
    "---\n",
    "例子 : 超级马里奥\n",
    "- 智能体\n",
    "  - 做动作或者做决策的主体:马里奥就是智能体\n",
    "- 环境\n",
    "  - 与智能体交互的对象:游戏程序就是环境\n",
    "- 状态(s)\n",
    "  - 在每个时刻，环境有一个状态：可以把马里奥当前的这一帧当成状态，\n",
    "- 观测\n",
    "  - 在每个时刻，状态并不能描述整个画面，例如，英雄联盟游戏中，你电脑当前的界面，并不能算一个状态，应为还有草里面的人，你观察不到，所以当前画面只能算观测\n",
    "- 状态空间(S)\n",
    "  - 所有可能存在状态的集合，可以是离散的，也可以是连续的；可以是有限集合，也可以是是无限集合，例如超级马里奥这款游戏就是无限集合，象棋这种就是有限集合，可以列举出所有的可能性\n",
    "- 动作(a)\n",
    "  - 智能体基于当前状态做出的决策，根据当前游戏画面，控制马里奥向左、向右或者跳跃\n",
    "- 动作空间(A)\n",
    "  - 所有可能的动作的集合，可以是有限的、也可以是无限的\n",
    "- 奖励(r)\n",
    "  - 指在智能体执行一个动作之后，环境返回给智能体的一个数字，例如，跳跃顶一个蘑菇，+50分，向右走，不顶蘑菇，+10分，碰到小怪，-100分\n",
    "  - 奖励函数：$r = (s,a,s^{'})$\n",
    "- 状态转移\n",
    "  - 指智能体从当前t时刻状态s转移到下一个时刻的状态$s^{'}$的过程，可以是随机的，也可以是确定的，例如操作马里奥根据当前的画面做出了相应的动作，进入下一个画面\n",
    "  - 状态转移函数:$p_t(s^{'}|s,a) = \\mathbb{P}(S^{'}_{t+1}=s^{'}|S_{t}=s,A_{t}=a)$\n",
    "- 策略($\\pi$)\n",
    "  - 根据当前的状态，从动作空间里面选取一个动作，可以是固定的，也可以是随机的\n",
    "  - 随机性策略：概率密度函数：$\\pi (a|s) = \\mathbb{P} (A=a,S=s)$,例如马里奥当中：\n",
    "    - $\\pi (左|s)=0.2$\n",
    "    - $\\pi (右|s)=0.7$\n",
    "    - $\\pi (跳|s)=0.1$\n",
    "  - 确定性策略：将状态输入之间得到动作而不是概率值\n",
    "- 智能体与环境交互\n",
    "  - 指智能体观测到环境的状态s，做出动作a,动作会改变环境的状态，环境反馈给智能体奖励r，以及新的状态$s^{'}$\n",
    "- 回合\n",
    "  - 指从游戏开始到结束的过程，例如，从马里奥出发到达终点或者死亡\n",
    "- 轨迹\n",
    "  - 在一个回合中，智能体的所有状态、动作、奖励\n",
    "\n",
    "- 随机性，随机性的来源有两个：动作和状态\n",
    "  - 动作的随机性来自策略\n",
    "  - 状态的随机性来自状态转移\n",
    "- 回报($U_t$)\n",
    "  - 从当前时刻开始到本回合结束所有的奖励的总和，也叫做累计奖励\n",
    "  - $U_t = R_t + R_{t+1} +R_{t+2}+\\cdots+R_{n}$\n",
    "- 折扣回报\n",
    "  - 对未来奖励进行打折扣\n",
    "  - $U_t = R_t + \\gamma \\cdot R_{t+1} +\\gamma^2 \\cdot R_{t+2}+\\cdots+ \\gamma^{n-1} \\cdot R_{n}$\n",
    "- 回报的随机性\n",
    "  - 由于未来的状态和动作都是随机的，奖励有基于状态和动作，所有回报也是随机的\n",
    "- 动作价值函数\n",
    "\n",
    "\n",
    "- 最优动作价值函数\n",
    "\n",
    "\n",
    "- 状态价值函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d8b4d",
   "metadata": {},
   "source": [
    "### 熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbeddd9",
   "metadata": {},
   "source": [
    "- 交叉熵,用作分类问题的损失函数\n",
    "  - $$P = {[p_1,\\dots,p_m]}^T , Q = {[q_1,\\dots,q_m]}^T$$\n",
    "  - 表示两个m为的离散概率分布，向量的元素均非负，且$\\sum_{j=1}^m p_j=1,\\sum_{j=1}^m q_j=1$,两者之间的熵定义为：\n",
    "  - $$H(P,Q) = -\\sum_{j=1}^m p_j \\cdot \\ln q_j$$\n",
    "- 熵：\n",
    "  - 熵是交叉熵的一个特例,简称H(P)\n",
    "  - $$H(P,P) = -\\sum_{j=1}^m p_j \\cdot \\ln p_j$$\n",
    "- 相对熵，也称KL散度，用于衡量两个概率分布之间的区别有多大：\n",
    "  - $$KL(P,Q) = \\sum_{j=1}^m p_j \\cdot \\ln \\frac {p_j}{q_j}$$\n",
    "  - KL散度总是非负，越接近0表示P,Q两者分布越相似\n",
    "- 从上面定义可得：\n",
    "  - $$KL(P,Q)= H(P,Q) - H(P)$$\n",
    "  - 由于熵$H(P)$不依赖于Q，因此一旦固定P，KL散度就等于交叉熵加上常数，如果P是固定的，那么关于Q的优化KL散度等价于优化交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6681bfa",
   "metadata": {},
   "source": [
    "### 蒙特卡洛\n",
    "- 通过生成大量随机样本，利用概率统计的结果来近似解决复杂问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b26b2",
   "metadata": {},
   "source": [
    "### 马尔可夫性\n",
    "- 未来状态的条件概率分布仅依赖于当前状态，而与过去状态无关\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
